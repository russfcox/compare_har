#!/usr/bin/env python3
"""
HAR File Comparison and Analysis Tool

This script analyzes and compares HTTP Archive (HAR) files to identify performance
differences between different environments or test runs. It extracts timing data,
identifies slow requests, and generates visualizations to help understand performance
characteristics.

HAR files contain detailed timing information about web requests including:
- DNS lookup time
- Connection establishment time
- SSL handshake time
- Request/response times
- Content download time

Usage: python compare-har.py file1.har file2.har [options]
"""

# Import required libraries for JSON parsing, data manipulation, and visualization
import json          # For parsing HAR files (which are in JSON format)
import argparse      # For command-line argument parsing
import pandas as pd  # For data manipulation and CSV export
import matplotlib.pyplot as plt  # For creating visualizations

def load_har(file_path):
    """
    Load and parse a HAR (HTTP Archive) file.
    
    HAR files are JSON format files that contain detailed information about
    web page loading performance, including network requests, timings, and responses.
    They are typically generated by browser developer tools or automated testing tools.
    
    Args:
        file_path (str): Path to the HAR file to load
        
    Returns:
        dict: Parsed HAR data as a Python dictionary
        
    Raises:
        FileNotFoundError: If the specified file doesn't exist
        json.JSONDecodeError: If the file is not valid JSON
    """
    # Open the file with UTF-8 encoding to handle international characters
    with open(file_path, 'r', encoding='utf-8') as f:
        # Parse the JSON content and return as a Python dictionary
        return json.load(f)

def extract_timings(entry):
    """
    Extract timing information from a single HAR entry.
    
    HAR timing structure includes several phases of an HTTP request:
    - blocked: Time spent in a queue waiting for a network connection
    - dns: DNS resolution time
    - connect: Time to establish TCP connection
    - send: Time to send HTTP request to server
    - wait: Time waiting for response from server (server processing time)
    - receive: Time to download the response from server
    
    Args:
        entry (dict): A single entry from the HAR file's entries array
        
    Returns:
        dict: Dictionary containing all timing phases plus calculated total time
    """
    # Extract the timings object from the entry
    t = entry['timings']
    
    # Return a dictionary with all timing phases
    # Use .get() with default 0 to handle missing timing values
    # Helper function to safely get timing values, handling None
    def safe_get(key):
        value = t.get(key, 0)
        return 0 if value is None else value
    
    return {
        'blocked': safe_get('blocked'),    # Queue/blocking time
        'dns': safe_get('dns'),            # DNS lookup time
        'connect': safe_get('connect'),    # Connection establishment time
        'send': safe_get('send'),          # Request send time
        'wait': safe_get('wait'),          # Server response wait time
        'receive': safe_get('receive'),    # Response download time
        # Calculate total time by summing all positive timing values
        # Filter out negative values and None values (which indicate unavailable timing data)
        'total': sum([v for v in t.values() if isinstance(v, (int, float)) and v >= 0])
    }

def compare_har_files(file1, file2, domain_filter=None, status_filter=None, top_n=10):
    """
    Compare timing data between two HAR files and generate analysis.
    
    This is the main analysis function that:
    1. Loads both HAR files
    2. Filters entries based on optional criteria
    3. Finds common URLs between both files
    4. Calculates timing differences
    5. Identifies slowest requests
    6. Exports results and creates visualizations
    
    Args:
        file1 (str): Path to the first HAR file (baseline)
        file2 (str): Path to the second HAR file (comparison)
        domain_filter (str, optional): Only analyze URLs containing this substring
        status_filter (int, optional): Only analyze requests with this HTTP status code
        top_n (int): Number of slowest requests to analyze and export (default: 10)
    """
    # Load both HAR files into memory
    print(f"Loading HAR files...")
    har1 = load_har(file1)
    har2 = load_har(file2)

    # Define a nested function to filter and process HAR entries
    def filtered_entries(har):
        """
        Filter HAR entries based on domain and status criteria.
        
        Returns a dictionary mapping URLs to their entry data, timings, and status.
        This allows for easy lookup and comparison between the two HAR files.
        """
        return {
            # Use the URL as the key for easy matching between files
            e['request']['url']: {
                'entry': e,                           # Full HAR entry data
                'timings': extract_timings(e),        # Extracted timing data
                'status': e['response']['status']     # HTTP status code
            }
            # Iterate through all entries in the HAR log
            for e in har['log']['entries']
            # Apply optional filters:
            # - Domain filter: URL must contain the specified substring
            # - Status filter: Response status must match exactly
            if (not domain_filter or domain_filter in e['request']['url']) and
               (not status_filter or e['response']['status'] == status_filter)
        }

    # Process entries from both HAR files with applied filters
    print(f"Processing entries with filters - Domain: {domain_filter}, Status: {status_filter}")
    entries1 = filtered_entries(har1)
    entries2 = filtered_entries(har2)

    # Find URLs that exist in both HAR files for comparison
    # Use set intersection to find common URLs
    common_urls = set(entries1.keys()) & set(entries2.keys())
    
    # Check if we have any URLs to compare
    if not common_urls:
        print("No common URLs found between HAR files after applying filters.")
        print("This could mean:")
        print("- The HAR files contain completely different requests")
        print("- The applied filters are too restrictive")
        print("- The URL formats differ between the files")
        return

    print(f"Comparing latency for {len(common_urls)} shared URLs:\n")

    # Initialize dictionaries to track cumulative timing differences
    # This will be used to calculate average differences across all requests
    total_diffs = {k: 0 for k in ['blocked', 'dns', 'connect', 'send', 'wait', 'receive', 'total']}
    diffs_list = []  # List to store detailed difference data for each URL

    # Iterate through each common URL and calculate timing differences
    for url in common_urls:
        # Get timing data for this URL from both HAR files
        t1 = entries1[url]['timings']  # Timings from first HAR file
        t2 = entries2[url]['timings']  # Timings from second HAR file
        
        # Calculate the difference for each timing phase (HAR2 - HAR1)
        # Positive values indicate HAR2 was slower, negative values indicate it was faster
        diff = {k: t2[k] - t1[k] for k in t1}

        # Add this URL's differences to the running totals
        for k in diff:
            total_diffs[k] += diff[k]

        # Store detailed information about this URL's performance difference
        diffs_list.append({
            'url': url,                                    # The URL being compared
            'total_diff': diff['total'],                   # Total timing difference
            'har1_total': t1['total'],                     # Total time in first HAR
            'har2_total': t2['total'],                     # Total time in second HAR
            'status1': entries1[url]['status'],            # HTTP status in first HAR
            'status2': entries2[url]['status'],            # HTTP status in second HAR
            'breakdown_diff': diff                         # Full breakdown of timing differences
        })

        # Print detailed comparison for this URL
        print(f"URL: {url}")
        print(f"  Total time: {t1['total']} ms (HAR1) → {t2['total']} ms (HAR2) | Δ {diff['total']:.2f} ms")
        print(f"  Breakdown: wait Δ {diff['wait']:.2f}, connect Δ {diff['connect']:.2f}, receive Δ {diff['receive']:.2f}\n")

    # Calculate and display average timing differences across all compared URLs
    count = len(common_urls)
    print("\nAverage Latency Differences (HAR2 - HAR1):")
    for k, v in total_diffs.items():
        avg_diff = v / count
        print(f"  {k}: {avg_diff:.2f} ms")

    # Sort requests by total timing difference (descending) to find the slowest
    # This identifies which requests had the biggest performance regression
    slowest = sorted(diffs_list, key=lambda x: x['total_diff'], reverse=True)[:top_n]

    # Display the top N slowest requests
    print(f"\nTop {top_n} Slowest Requests (by latency increase):")
    for i, item in enumerate(slowest, 1):
        print(f"{i}. {item['url']}")
        print(f"   HAR1: {item['har1_total']} ms → HAR2: {item['har2_total']} ms | Δ {item['total_diff']:.2f} ms")

    # Export results to CSV and JSON formats for further analysis
    print(f"\nExporting top {top_n} slowest requests...")
    
    # Create a pandas DataFrame for easy CSV export
    # Only include the most relevant columns for the CSV
    export_df = pd.DataFrame(slowest)
    export_df[['url', 'har1_total', 'har2_total', 'total_diff']].to_csv('slowest_requests.csv', index=False)
    
    # Export full detailed data to JSON format
    with open('slowest_requests.json', 'w', encoding='utf-8') as f:
        json.dump(slowest, f, indent=2)

    print(f"Top {top_n} slowest requests exported to:")
    print("  - slowest_requests.csv")
    print("  - slowest_requests.json")

    # Create a horizontal bar chart visualization of latency differences
    print("Generating visualization...")
    
    # Extract data for plotting
    urls = [x['url'] for x in slowest]      # URL list for y-axis labels
    deltas = [x['total_diff'] for x in slowest]  # Timing differences for x-axis values

    # Create the plot
    plt.figure(figsize=(12, 6))  # Set figure size for readability
    
    # Create horizontal bar chart
    # range(len(urls)) creates y-positions for each bar
    plt.barh(range(len(urls)), deltas)
    
    # Set y-axis labels to show numbered URLs
    # Enumerate to add numbers (1., 2., etc.) for easy reference
    plt.yticks(range(len(urls)), [f"{i+1}. {url}" for i, url in enumerate(urls)])
    
    # Set axis labels and title
    plt.xlabel("Latency Increase (ms)")
    plt.title(f"Top {top_n} Slowest Requests (HAR2 - HAR1)")
    
    # Invert y-axis so the slowest request appears at the top
    plt.gca().invert_yaxis()
    
    # Adjust layout to prevent label cutoff
    plt.tight_layout()
    
    # Save the plot to a file
    plt.savefig("latency_deltas.png")
    
    # Display the plot (if running in an interactive environment)
    plt.show()

    print("Latency delta bar chart saved to: latency_deltas.png")

# Main execution block - only runs when script is executed directly (not imported)
if __name__ == "__main__":
    # Set up command-line argument parsing
    parser = argparse.ArgumentParser(
        description="Compare latency between two HAR files with export and visualization",
        epilog="""
        Example usage:
          python compare-har.py baseline.har test.har
          python compare-har.py baseline.har test.har --filter-domain api.example.com
          python compare-har.py baseline.har test.har --filter-status 200 --top-n 20
        """
    )
    
    # Define required positional arguments
    parser.add_argument("file1", help="Path to the first HAR file (baseline)")
    parser.add_argument("file2", help="Path to the second HAR file (comparison)")
    
    # Define optional arguments for filtering and customization
    parser.add_argument("--filter-domain",
                       help="Only include requests with this domain substring")
    parser.add_argument("--filter-status", type=int,
                       help="Only include requests with this HTTP status code")
    parser.add_argument("--top-n", type=int, default=10,
                       help="Number of slowest requests to analyse (default: 10)")
    
    # Parse the command-line arguments
    args = parser.parse_args()
    
    # Print the parsed arguments for debugging/confirmation
    print("Running HAR comparison with arguments:")
    print(vars(args))
    print("-" * 50)
    
    # Execute the main comparison function with parsed arguments
    compare_har_files(
        args.file1,
        args.file2,
        domain_filter=args.filter_domain,
        status_filter=args.filter_status,
        top_n=args.top_n
    )